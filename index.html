<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yansong Qu</title>
  
  <meta name="author" content="Yansong Qu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yansong Qu | 曲延松</name>
              </p>
              <p> 
                I am a second-year Ph.D student in the Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University </a>, advised by Prof. <a href="https://mac.xmu.edu.cn/rrji/">Rongrong Ji</a>. 
              </p>
              <p>
                My research interest lies in the <b>Machine Learning</b> and <b>3D Computer Vision</b>.
              </p>
              <p style="text-align:center">
                <a href="mailto:quyans@stu.xmu.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://Quyans.github.io/">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=zBLDzs4AAAAJ"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Quyans"> Github </a> 
              </p>
            </td>
            <td style="padding:3%;width:40%;max-width:40%">
              <img style="width:70%;max-width:70%" alt="profile photo" src="images/qys01.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2024-09:</b> Two papers are accepted by <a href="https://neurips.cc/Conferences/2024">NeurIPS 2024</a>. Congratulate to Xinyang, YunPeng!
              </li>
	            <li style="margin: 5px;" >
              <b>2024-07:</b> One paper on 3D Perception is accepted by <a href="https://2024.acmmm.org/">ACMMM 2024</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-04:</b> One paper on 3D Editing is accepted by <a href="#">ICMR 2023</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-01:</b> One paper on 3D Perception And Reconstruction is accepted by <a href="#">ICME 2023</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/dyg.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Drag Your Gaussian: Effective Point-Based Editing with Score Distillation for 3D Gaussian Splatting</papertitle>
              <br>
              <strong> Yansong Qu </strong>, 
              <a href="">Dian Chen</a>,
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=M9rwkHwAAAAJ">Xinyang Li</a>,
              <a href="">Xiaofan Li</a>,
              <a href="https://scholar.google.com.hk/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao"> Shengchuan Zhang</a>, 
              <a href="https://scholar.google.com.hk/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao"> Liujuan Cao</a>, 
              <a href="https://scholar.google.com.hk/citations?user=lRSD7PQAAAAJ&hl=zh-CN&oi=ao"> Rongrong Ji</a>
              <br>
              <em>ArXiv, 2025</em>
              <br>
              <a href="https://quyans.github.io/Drag-Your-Gaussian/">[Project Page]</a>
              <a href="https://arxiv.org/abs/2501.18672">[arXiv]</a>
              <a href="https://github.com/Quyans/Drag-Your-Gaussian">[Code]</a>
              <br>
              <!-- <p> In this work, we delve into the key challenge of the complex and scene-specific camera trajectories found in real-world captures. We introduce Director3D, a robust open-world text-to-3D generation framework, designed to generate both real-world 3D scenes and adaptive camera trajectories. To achieve this, (1) we first utilize a Trajectory Diffusion Transformer, acting as the Cinematographer, to model the distribution of camera trajectories based on textual descriptions. Next, a Gaussian-driven Multi-view Latent Diffusion Model serves as the Decorator, modeling the image sequence distribution given the camera trajectories and texts. This model, fine-tuned from a 2D diffusion model, directly generates pixel-aligned 3D Gaussians as an immediate 3D scene representation for consistent denoising. Lastly, the 3D Gaussians are further refined by a novel SDS++ loss as the Detailer, which incorporates the prior of the 2D diffusion model.</p> -->
            </td>
          </tr>

          <tr>
            <td colspan="2" style="padding-bottom: 15px;">
                <div style="border-bottom: 2px solid #ccc; width: 100%;"></div>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/cross-attach.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Cross-Modality Perturbation Synergy Attack for Person Re-identification</papertitle>
              <br>
              <a href=""> Yunpeng Gong </a>, 
              <a href="">Zhun Zhong</a>,
              <a href="">Zhiming Luo</a>,
              <strong>Yansong Qu</strong>, 
              <a href="https://scholar.google.com.hk/citations?user=lRSD7PQAAAAJ&hl=zh-CN&oi=ao"> Rongrong Ji</a>, 
              <a href=""> Min Jiang</a>
              <br>
              <em>NeurIPS, 2024</em>
              <br>
              <!-- <a href="https://imlixinyang.github.io/director3d-page/">[Project Page]</a> -->
              <a href="https://arxiv.org/pdf/2401.10090">[arXiv]</a>
              <!-- <a href="https://github.com/imlixinyang/director3d">[Code]</a> -->
              <br>
              <!-- <p> In this work, we delve into the key challenge of the complex and scene-specific camera trajectories found in real-world captures. We introduce Director3D, a robust open-world text-to-3D generation framework, designed to generate both real-world 3D scenes and adaptive camera trajectories. To achieve this, (1) we first utilize a Trajectory Diffusion Transformer, acting as the Cinematographer, to model the distribution of camera trajectories based on textual descriptions. Next, a Gaussian-driven Multi-view Latent Diffusion Model serves as the Decorator, modeling the image sequence distribution given the camera trajectories and texts. This model, fine-tuned from a 2D diffusion model, directly generates pixel-aligned 3D Gaussians as an immediate 3D scene representation for consistent denoising. Lastly, the 3D Gaussians are further refined by a novel SDS++ loss as the Detailer, which incorporates the prior of the 2D diffusion model.</p> -->
            </td>
          </tr>

          <tr>
            <td colspan="2" style="padding-bottom: 15px;">
                <div style="border-bottom: 2px solid #ccc; width: 100%;"></div>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/director3d.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Director3D: Real-world Camera Trajectory and 3D Scene Generation from Text</papertitle>
              <br>
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=M9rwkHwAAAAJ"> Xinyang Li </a>, 
              <a href="https://github.com/Clear-3d">Zhangyu Lai</a>,
              <a href="https://eveneveno.github.io/lnxu/">Linning Xu</a>,
              <strong>Yansong Qu</strong>, 
              <a href="https://scholar.google.com.hk/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao"> Liujuan Cao</a>, 
              <a href="https://scholar.google.com.hk/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao"> Shengchuan Zhang</a>, 
              <a href="https://daibo.info/"> Bo Dai</a>, 
              <a href="https://scholar.google.com.hk/citations?user=lRSD7PQAAAAJ&hl=zh-CN&oi=ao"> Rongrong Ji</a>
              <br>
              <em>NeurIPS, 2024</em>
              <br>
              <a href="https://imlixinyang.github.io/director3d-page/">[Project Page]</a>
              <a href="https://arxiv.org/pdf/2406.17601">[arXiv]</a>
              <a href="https://github.com/imlixinyang/director3d">[Code]</a>
              <br>
              <!-- <p> In this work, we delve into the key challenge of the complex and scene-specific camera trajectories found in real-world captures. We introduce Director3D, a robust open-world text-to-3D generation framework, designed to generate both real-world 3D scenes and adaptive camera trajectories. To achieve this, (1) we first utilize a Trajectory Diffusion Transformer, acting as the Cinematographer, to model the distribution of camera trajectories based on textual descriptions. Next, a Gaussian-driven Multi-view Latent Diffusion Model serves as the Decorator, modeling the image sequence distribution given the camera trajectories and texts. This model, fine-tuned from a 2D diffusion model, directly generates pixel-aligned 3D Gaussians as an immediate 3D scene representation for consistent denoising. Lastly, the 3D Gaussians are further refined by a novel SDS++ loss as the Detailer, which incorporates the prior of the 2D diffusion model.</p> -->
            </td>
          </tr>

          <tr>
            <td colspan="2" style="padding-bottom: 15px;">
                <div style="border-bottom: 2px solid #ccc; width: 100%;"></div>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/goi.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GOI: Find 3D Gaussians of Interest with an Optimizable Open-vocabulary Semantic-space Hyperplane</papertitle>
              <br>
              <strong>Yansong Qu*</strong>, 
              Shaohui Dai* , 
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=M9rwkHwAAAAJ"> Xinyang Li </a>, 
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=hxZ-5AkAAAAJ">Jianghang Lin </a>, 
              <a href="https://scholar.google.com.hk/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao"> Liujuan Cao</a>, 
              <a href="https://scholar.google.com.hk/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao"> Shengchuan Zhang</a>, 
              <a href="https://scholar.google.com.hk/citations?user=lRSD7PQAAAAJ&hl=zh-CN&oi=ao"> Rongrong Ji</a>
              <br>
              <em>ACMMM, 2024</em>
              <br>
              <a href="https://quyans.github.io/GOI-Hyperplane/">[Project Page]</a>
              <a href="https://arxiv.org/abs/2406.04338">[arXiv]</a>
              <a href="https://github.com/Quyans/GOI-Hyperplane/">[Code]</a>
              <br>
              <!-- <p> In this paper, we propose GOI, a novel method for 3D open-vocabulary scene understanding. Our approach includes an efficient compression method that utilizes scene priors to condense noisy high-dimensional semantic features into compact low-dimensional vectors, which are subsequently embedded in 3DGS. And we leverage an off-the-shelf 2D Referring Expression Segmentation (RES) model to fine-tune the semantic-space hyperplane, enabling a more precise distinction between target regions and others. </p> -->
            </td>
          </tr>

          <tr>
            <td colspan="2" style="padding-bottom: 15px;">
                <div style="border-bottom: 2px solid #ccc; width: 100%;"></div>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/sgnerf2.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SG-NeRF: Semantic-guided Point-based Neural Radiance Fields</papertitle>
              <br>
              <strong>Yansong Qu*</strong>, 
              <a href="https://scholar.google.com.hk/citations?user=V_5VPiEAAAAJ&hl=zh-CN&oi=sra">Yuze Wang*</a>,  
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=Q-UkdjUAAAAJ">Yue Qi </a>
              <br>
              <em>ICME, 2023</em>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/10219715">[Paper]</a>
              <a href="https://github.com/Quyans/SG-NeRF">[Code]</a>
              <br>
              <!-- <p> Neural Radiance Fields (NeRF) require a large number of high-quality images to achieve novel view synthesis in room-scale scenes, but capturing these images is very labor-intensive. To address this, we propose Semantic-Guided Point-Based NeRF (SG-NeRF), which can reconstruct the radiance field using only a few images. We leverage sparse 3D point clouds with neural features as geometry constraints for NeRF optimization and use semantic predictions from both 2D images and 3D point clouds to guide the search for neighboring neural points during ray marching. This semantic guidance allows the sampled points to accurately find structurally related points even in large areas with unevenly distributed sparse point clouds, enabling high-quality rendering with fewer input images. </p> -->
            </td>
          </tr>
          
          <tr>
            <td colspan="2" style="padding-bottom: 15px;">
                <div style="border-bottom: 2px solid #ccc; width: 100%;"></div>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/ripnerf.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>RIP-NeRF: Learning Rotation-Invariant Point-based Neural
                Radiance Field for Fine-grained Editing and Compositing</papertitle>
              <br>
              <a href="https://scholar.google.com.hk/citations?user=V_5VPiEAAAAJ&hl=zh-CN&oi=sra">Yuze Wang</a>, 
              <a href="">Junyi Wang</a>, 
              <strong>Yansong Qu</strong>, 
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=Q-UkdjUAAAAJ">Yue Qi </a> 
              <br>
              <em>ICMR, 2023</em>
              <br>
              <a href="https://dl.acm.org/doi/abs/10.1145/3591106.3592276">[Paper]</a>
              <br>
              <!-- <p>In this work, we introduce Rotation-Invariant Point-based NeRF (RIP-NeRF), combining the strengths of implicit NeRF-based and explicit point-based representations for fine-grained editing and cross-scene compositing of radiance fields. We replace the traditional Cartesian coordinates with a novel rotation-invariant point-based radiance field representation, using a Neural Inverse Distance Weighting Interpolation (NIDWI) module to enhance rendering quality. For cross-scene compositing, we disentangle the rendering module from the neural point-based representation, allowing controllable compositing across scenes without the need for retraining.</p> -->
            </td>
          </tr>

          <!-- <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DreamReward.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DreamReward: Text-to-3D Generation with Human Preference</papertitle>
              <br>
              <a href="https://jamesyjl.github.io/"> Junliang Ye* </a>, 
              <strong>Fangfu Liu*</strong>, 
              Qixiu Li,
              <a href="https://thuwzy.github.io/"> Zhengyi Wang </a>,
              <a href="https://yikaiw.github.io/"> Yikai Wang </a>,
              <br>
              Xinzhou Wang,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml"> Jun Zhu </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2403.14613">[arXiv]</a>
              <a href="https://github.com/liuff19/DreamReward">[Code]</a>
              <a href="https://jamesyjl.github.io/DreamReward/">[Project Page]</a> 
              <br>
              <p> In this work, We propose the first general-purpose human preference reward model for text-to-3D generation, named Reward3D. Then we further introduce a novel text-to-3D framework, coined DreamReward, which greatly boosts high-text alignment and high-quality 3D generation through human preference feedback. </p>
            </td>
          </tr> -->


        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <!-- <li style="margin: 5px;"> 
                Act as a reviewer for <b>TCSVT</b>, <b>ICLR 2025</b>, <b>NeurIPS 2024</b>, <b>ICMR 2024</b>.
              </li> -->
            <H3 >
              Conference Review:
            </H3>
            <li style="margin: 5px;">International Conference on Machine Learning (<b>ICML</b>) 2025.</li>
            <li style="margin: 5px;">IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2025.</li>
            <li style="margin: 5px;">International Conference on Learning Representations (<b>ICLR</b>) 2025.</li>
            <li style="margin: 5px;">Conference and Workshop on Neural Information Processing Systems (<b>NeurIPS</b>) 2024.</li>
            <!-- <li style="margin: 5px;">International Conference on Multimedia Retrieval (<b>ICMR</b>) 2024.</li> -->
            <h3 >
              <b>Journal Review:</b>
            </h3>
            <li style="margin: 5px;">IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>).</li>
          </td>
        </tr>
      </tbody></table>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Honors and Awards</heading>
          <p>
            <!-- <li style="margin: 5px;"> <b>National Scholarship (Top 1 in 260+ in the 2019-2020 academic year)</b></li> -->
            <li style="margin: 5px;"> Beihang University Outstanding Graduate 2023</li>
            <li style="margin: 5px;"> Shandong University First-Class Scholarship, twice, 2018, 2019</li>
            <li style="margin: 5px;"> Special Prize in the Qilu Software Competition 2019</li>
          </p>
        </td>
      </tr>
    </tbody></table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://liuff19.github.io/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

<p><center>
	  <div id="clustrmaps-widget" style="width:10%">
      <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=TTpsNidh1pv-OGyMcBPr76ga5OnV9DKE3-KgEFRFJ_8"></script>
	  <br>
      <p>&copy; Yansong Qu</p>
      <p>Last updated: 10 Oct, 2024</p>
</center></p>
</body>

</html>
