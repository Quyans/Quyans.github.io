<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yansong Qu</title>
  
  <meta name="author" content="Yansong Qu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yansong Qu | 曲延松</name>
              </p>
              <p> 
                I am a first-year Ph.D student in the Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University </a>, advised by Prof. <a href="https://mac.xmu.edu.cn/rrji/">Rongrong Ji</a>. 
              </p>
              <p>
                My research interest lies in the <b>Machine Learning</b> and <b>3D Computer Vision</b>.
              </p>
              <p style="text-align:center">
                <a href="mailto:quyans@stu.xmu.edu.com">Email</a> &nbsp/&nbsp
                <a href="https://Quyans.github.io/">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=zBLDzs4AAAAJ"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Quyans"> Github </a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:3%;width:40%;max-width:40%">
              <img style="width:70%;max-width:70%" alt="profile photo" src="images/qys01.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
	      <li style="margin: 5px;" >
              <b>2024-07:</b> One paper on 3D Perception is accepted by <a href="https://2024.acmmm.org/">ACMMM 2024</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-04:</b> One paper on 3D Reconstruction is accepted by <a href="https://cvpr.thecvf.com/">ICMR 2023</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2022-11:</b> One paper on 3D Perception is accepted by <a href="https://cvpr.thecvf.com/">ICME 2023</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/goi.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GOI: Find 3D Gaussians of Interest with an Optimizable Open-vocabulary Semantic-space Hyperplane</papertitle>
              <br>
              <strong>Yansong Qu*</strong>, 
              Shaohui Dai*
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=M9rwkHwAAAAJ"> Xinyang Li </a>,
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=hxZ-5AkAAAAJ">Jianghang Lin </a>,
              <a href="https://scholar.google.com.hk/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao"> Liujuan Cao</a>,
              <a href="https://scholar.google.com.hk/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao"> Shengchuan Zhang</a>
              <a href="https://scholar.google.com.hk/citations?user=lRSD7PQAAAAJ&hl=zh-CN&oi=ao"> Rongrong Ji</a>
              <br>
              <em>ACMMM, 2024</em>
              <br>
              <a href="https://arxiv.org/abs/2406.04338">[arXiv]</a>
              <a href="https://quyans.github.io/GOI-Hyperplane/">[Code]</a>
              <a href="https://github.com/Quyans/GOI-Hyperplane">[Project Page]</a> 
              <br>
              <p> In this paper, we propose GOI, a novel method for 3D open-vocabulary scene understanding. Our approach includes an efficient compression method that utilizes scene priors to condense noisy high-dimensional semantic features into compact low-dimensional vectors, which are subsequently embedded in 3DGS. And we leverage an off-the-shelf 2D Referring Expression Segmentation (RES) model to fine-tune the semantic-space hyperplane, enabling a more precise distinction between target regions and others. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Unique3d.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=VTU0gysAAAAJ&hl=zh-CN&oi=ao"> Kailu Wu </a>, 
              <strong>Fangfu Liu</strong>, 
              Zhihan Cai, 
              Runjie Yan, 
              Hanyang Wang, 
              Yating Hu,
              <br>
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <a href="https://group.iiis.tsinghua.edu.cn/~maks/"> Kaisheng Ma </a>
              <br>
              <em>Arxiv, 2024</em>
              <br>
              <a href="https://arxiv.org/abs/2405.20343">[arXiv]</a>
              <a href="https://github.com/AiuniAI/Unique3D">[Code]</a>
              <a href="https://wukailu.github.io/Unique3D/">[Project Page]</a> 
              <br>
              <p> In this work, we introduce Unique3D, a novel image-to-3D framework for efficiently generating high-quality 3D meshes from single-view images, featuring state-of-the-art generation fidelity and strong generalizability. Unique3D can generate a high-fidelity textured mesh from a single orthogonal RGB image of any object in under 30 seconds.  </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DreamReward.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DreamReward: Text-to-3D Generation with Human Preference</papertitle>
              <br>
              <a href="https://jamesyjl.github.io/"> Junliang Ye* </a>, 
              <strong>Fangfu Liu*</strong>, 
              Qixiu Li,
              <a href="https://thuwzy.github.io/"> Zhengyi Wang </a>,
              <a href="https://yikaiw.github.io/"> Yikai Wang </a>,
              <br>
              Xinzhou Wang,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml"> Jun Zhu </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2403.14613">[arXiv]</a>
              <a href="https://github.com/liuff19/DreamReward">[Code]</a>
              <a href="https://jamesyjl.github.io/DreamReward/">[Project Page]</a> 
              <br>
              <p> In this work, We propose the first general-purpose human preference reward model for text-to-3D generation, named Reward3D. Then we further introduce a novel text-to-3D framework, coined DreamReward, which greatly boosts high-text alignment and high-quality 3D generation through human preference feedback. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/makeyour3d.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation</papertitle>
              <br>
              <strong>Fangfu Liu</strong>, 
              Hanyang Wang, 
              Weiliang Chen,
              Haowen Sun,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2403.09625">[arXiv]</a>
              <a href="https://github.com/liuff19/Make-Your-3D">[Code]</a>
              <a href="https://liuff19.github.io/Make-Your-3D/">[Project Page]</a> 
              <br>
              <p> We introduce a novel 3D customization method, dubbed Make-Your-3D that can personalize high-fidelity and consistent 3D content from only a single image of a subject with text description within 5 
                  minutes. </p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Sherpa.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior</papertitle>
              <br>
              <strong>Fangfu Liu</strong>, 
              Diankun Wu, 
              <a href="https://weiyithu.github.io/"> Yi Wei </a>, 
              <a href="https://raoyongming.github.io/"> Yongming Rao </a>,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2312.06655">[arXiv]</a>
              <a href="https://github.com/liuff19/Sherpa3D">[Code]</a>
              <a href="https://liuff19.github.io/Sherpa3D/">[Project Page]</a> 
              <br>
              <p> We propose Sherpa3D, a new text-to-3D framework that achieves high-fidelity, generalizability, and geometric consistency simultaneously. Extensive experiments show the superiority of our Sherpa3D over the state-of-the-art text-to-3D methods in terms of quality and 3D consistency.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/CASPER.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Discovering Dynamic Causal Space for DAG Structure Learning</papertitle>
              <br>
              <strong>Fangfu Liu</strong>, 
              Wenchang Ma, 
              <a href="https://anzhang314.github.io/index.html"> An Zhang </a>, 
              <a href="https://xiangwang1223.github.io/"> Xiang Wang </a>,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <a href="https://scholar.google.com/citations?user=Z9DWCBEAAAAJ&hl=en&oi=ao"> Tat-Seng Chua </a>
              <br>
              <em>ACM SIGKDD Conference on Knowledge Discovery and Data Mining (<strong>KDD</strong>)</em>, 2023
              <br>
              <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2306.02822">[arXiv]</a>
              <a href="https://arxiv.org/abs/2306.02822">[Code]</a>
              <a href="https://arxiv.org/abs/2306.02822">[Project Page]</a> 
              <br>
              <p> we propose a dynamic causal space for DAG structure learning, coined CASPER, that integrates the graph structure into the score function as a new measure in the causal space to faithfully reflect the causal distance between estimated and groundtruth DAG.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/S-Ray.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Semantic Ray: Learning a Generalizable Semantic Field with Cross-Reprojection Attention</papertitle>
              <br>
              <strong>Fangfu Liu</strong>, 
              Chubin Zhang, 
              <a href="https://yzheng97.github.io/"> Yu Zheng</a>, 
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.13014">[arXiv]</a>
              <a href="https://github.com/liuff19/Semantic-Ray">[Code]</a>
              <a href="https://liuff19.github.io/S-Ray/">[Project Page]</a> 
              <br>
              <p> We propose a neural semantic representation called Semantic-Ray (S-Ray) to build a generalizable semantic field, which is able to learn from multiple scenes and directly infer semantics on novel viewpoints across novel scenes.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/rescore.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Boosting Differentiable Causal Discovery via Adaptive Sample Reweighting</papertitle>
              <br>
              <a href="https://anzhang314.github.io/index.html"> An Zhang</a>,
              <strong>Fangfu Liu</strong>, 
              Wenchang Ma,
              Zhibo Cai,
              <a href="https://xiangwang1223.github.io/"> Xiang Wang </a>,
              <a href="https://scholar.google.com/citations?user=Z9DWCBEAAAAJ&hl=en&oi=ao"> Tat-Seng Chua </a>
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.03187">[arXiv]</a>
              <a href="https://github.com/anzhang314/ReScore">[Code]</a>
              <a href="https://arxiv.org/abs/2303.03187">[Project Page]</a> 
              <br>
              <p> We propose ReScore, a simple-yet-effective model-agnostic optimzation framework that simultaneously eliminates spurious edge learning and generalizes to heterogeneous data by utilizing learnable adaptive weights.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/VL-grasp.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>VL-Grasp: a 6-Dof Interactive Grasp Policy for Language-Oriented Objects in Cluttered Indoor Scenes</papertitle>
              <br>
              Yuhao Lu,
              Yixuan Fan,
              Beixing Deng,
              <strong>Fangfu Liu</strong>, 
              Yali Li,
              <a href="https://scholar.google.com/citations?user=RgzLZZsAAAAJ&hl=en&oi=ao"> Shengjin Wang </a>
              <br>
              <em>International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2308.00640">[arXiv]</a>
              <a href="https://github.com/luyh20/VL-Grasp">[Code]</a>
              <a href="https://arxiv.org/abs/2308.00640">[Project Page]</a> 
              <br>
              <p> The VL-Grasp is an interactive grasp policy combined with visual grounding and 6-dof grasp pose detection tasks. The robot can adapt to various observation views and more diverse indoor scenes to grasp the target according to a human's language command by applying the VL-Grasp. Meanwhile, we build a new visual grounding dataset specially designed for the robot interaction grasp task, called RoboRefIt.</p>
            </td>
          </tr>



        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> <b>National Scholarship (Top 1 in 260+ in the 2019-2020 academic year)</b></li>
                <li style="margin: 5px;"> Tsinghua University Comprehensive Excellent Award <b>twice (Top 5% in 260+, 2020&2021)</b></li>
                <li style="margin: 5px;"> Tsinghua Science and Technology Innovation Excellence Award (2022)</li>
                <li style="margin: 5px;"> Four star Bauhinia volunteer of Tsinghua University (Volunteer hours up to 150, 2021)</li>
                <li style="margin: 5px;"> Advanced individual award of Tsinghua University (2019)</li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                Review for <b>PRCV 2024</b>, <b>TCSVT</b>, <b>NeurIPS 2022</b>, <b>IROS 2023</b>, <b>ACMMM 2024</b>, <b>NeurIPS 2024</b>.
              </li>
          </td>
        </tr>
      </tbody></table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
      <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=B_hoqUcZkAVteexiuKv_tIvNw9enA1g2tIC3ypxXP2E"></script>
	  <br>
	    &copy; Fangfu Liu | Last updated: 02 July, 2024
</center></p>
</body>

</html>
